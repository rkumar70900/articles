{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1416471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajes\\OneDrive\\Documents\\GitHub\\articles\\venv312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import safetensors\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ce63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model id\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# checkpoint after QLoRA training\n",
    "checkpoint_path = \"./qlora_results/checkpoint-75\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6281c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for loading the model in 4-bit quantized form with NF4 quantization format\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f865979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model with the above quantization config\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201912bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the fine tuned QLoRA model from checkpoint\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0899e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 448 adapter weights\n"
     ]
    }
   ],
   "source": [
    "# safetensors file\n",
    "adapter_file = f\"{checkpoint_path}/adapter_model.safetensors\"\n",
    "# reading safetensors file\n",
    "with safetensors.safe_open(adapter_file, framework=\"pt\") as f:\n",
    "    adapter_weights = {}\n",
    "    for k in f.keys():\n",
    "        adapter_weights[k] = f.get_tensor(k)\n",
    "\n",
    "print(\"found {} adapter weights\".format(len(adapter_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e6b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter groups: 224\n",
      "\n",
      "Adapter Group 1: base_model.model.model.layers.0.mlp.down_proj\n",
      "LoRA-A (down-projection):\n",
      "Shape: torch.Size([64, 14336])\n",
      "Dtype: torch.float32\n",
      "LoRA-B (up-projection):\n",
      "Shape: torch.Size([4096, 64])\n",
      "Dtype: torch.float32\n",
      "\n",
      "Adapter Group 2: base_model.model.model.layers.0.mlp.gate_proj\n",
      "LoRA-A (down-projection):\n",
      "Shape: torch.Size([64, 4096])\n",
      "Dtype: torch.float32\n",
      "LoRA-B (up-projection):\n",
      "Shape: torch.Size([14336, 64])\n",
      "Dtype: torch.float32\n",
      "\n",
      "Adapter Group 3: base_model.model.model.layers.0.mlp.up_proj\n",
      "LoRA-A (down-projection):\n",
      "Shape: torch.Size([64, 4096])\n",
      "Dtype: torch.float32\n",
      "LoRA-B (up-projection):\n",
      "Shape: torch.Size([14336, 64])\n",
      "Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# inspecting adapter weights from checkpoint by lora_A and lora_B\n",
    "adapter_groups = {}\n",
    "for name, tensor in adapter_weights.items():\n",
    "    parts = name.split('.')\n",
    "    if 'lora_A' in name or 'lora_B' in name:\n",
    "        base_name = '.'.join(parts[:-2])\n",
    "        if base_name not in adapter_groups:\n",
    "            adapter_groups[base_name] = {}\n",
    "        adapter_groups[base_name][parts[-2]] = tensor\n",
    "\n",
    "# Number of adapter groups\n",
    "print(f\"Adapter groups: {len(adapter_groups)}\")\n",
    "\n",
    "# printing information for first three adapter groups\n",
    "for i, (module_name, adapters) in enumerate(list(adapter_groups.items())[:3]):\n",
    "    print(f\"\\nAdapter Group {i+1}: {module_name}\")\n",
    "    if 'lora_A' in adapters:\n",
    "        lora_A = adapters['lora_A']\n",
    "        print(f\"LoRA-A (down-projection):\")\n",
    "        print(f\"Shape: {lora_A.shape}\")\n",
    "        print(f\"Dtype: {lora_A.dtype}\")\n",
    "    if 'lora_B' in adapters:\n",
    "        lora_B = adapters['lora_B']\n",
    "        print(f\"LoRA-B (up-projection):\")\n",
    "        print(f\"Shape: {lora_B.shape}\")\n",
    "        print(f\"Dtype: {lora_B.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9333ac",
   "metadata": {},
   "source": [
    "### Explanation on why we see int8 weights when we loaded the base model as int4\n",
    "\n",
    "If you observe in the output we see in the weight type is int8 but we loaded the base model as int4. this is a common point of confusion when looking at the low-level memory details. \n",
    "\n",
    "The reason the weights appear as 8-bit integers even though we loaded the model with a 4-bit configuration is due to a technique called data packing, which is necessary because of how computer hardware and libraries like PyTorch handle memory.\n",
    "\n",
    "The main problem is that computer hardware and programming libraries like PyTorch typically cannot address or store data in units smaller than an 8-bit (torch.unit8).\n",
    "\n",
    "The smallest slot in memory you can put data into is an 8-bit unit. If the system stored a single 4-bit number in a full 8-bit slot, we would be wasting half of the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62304ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized Layer 1: model.layers.0.self_attn.q_proj.base_layer\n",
      "Weight shape: torch.Size([8388608, 1])\n",
      "Weight dtype: torch.uint8\n",
      "Weight storage: torch.uint8\n",
      "Effective bits per weight: 4.00\n",
      "Memory (fp32 equivalent): 32768.00 KB\n",
      "Memory (quantized): 8192.00 KB\n",
      "Compression ratio: 4.0x\n",
      "\n",
      "Quantized Layer 2: model.layers.0.self_attn.k_proj.base_layer\n",
      "Weight shape: torch.Size([2097152, 1])\n",
      "Weight dtype: torch.uint8\n",
      "Weight storage: torch.uint8\n",
      "Effective bits per weight: 4.00\n",
      "Memory (fp32 equivalent): 8192.00 KB\n",
      "Memory (quantized): 2048.00 KB\n",
      "Compression ratio: 4.0x\n",
      "\n",
      "Quantized Layer 3: model.layers.0.self_attn.v_proj.base_layer\n",
      "Weight shape: torch.Size([2097152, 1])\n",
      "Weight dtype: torch.uint8\n",
      "Weight storage: torch.uint8\n",
      "Effective bits per weight: 4.00\n",
      "Memory (fp32 equivalent): 8192.00 KB\n",
      "Memory (quantized): 2048.00 KB\n",
      "Compression ratio: 4.0x\n"
     ]
    }
   ],
   "source": [
    "# loading base model\n",
    "base_model = model.get_base_model()\n",
    "\n",
    "# checking quantized layers\n",
    "quantized_layers = []\n",
    "for name, module in base_model.named_modules():\n",
    "    if hasattr(module, 'weight') and hasattr(module, 'quant_state'):\n",
    "        quantized_layers.append((name, module))\n",
    "\n",
    "for i, (name, module) in enumerate(quantized_layers[:3]):\n",
    "    print(f\"\\nQuantized Layer {i+1}: {name}\")\n",
    "    \n",
    "    weight = module.weight\n",
    "    print(f\"Weight shape: {weight.shape}\")\n",
    "    print(f\"Weight dtype: {weight.dtype}\")\n",
    "    print(f\"Weight storage: {weight.data.dtype if hasattr(weight, 'data') else 'N/A'}\")\n",
    "\n",
    "    storage_elements = weight.data.numel() \n",
    "\n",
    "    total_logical_elements = storage_elements * 2 \n",
    "\n",
    "    storage_bits = storage_elements * 8\n",
    "\n",
    "    effective_bits = storage_bits / total_logical_elements\n",
    "\n",
    "    print(f\"Effective bits per weight: {effective_bits:.2f}\")\n",
    "    \n",
    "    weight_memory = weight.numel() * 4\n",
    "    quantized_memory = weight.data.numel() * 1 if hasattr(weight, 'data') else weight_memory\n",
    "    print(f\"Memory (fp32 equivalent): {weight_memory / 1024:.2f} KB\")\n",
    "    print(f\"Memory (quantized): {quantized_memory / 1024:.2f} KB\")\n",
    "    print(f\"Compression ratio: {weight_memory / quantized_memory:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f4c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY USAGE COMPARISON\n",
      "Memory Usage:\n",
      "4-bit base model: 3828.51 MB\n",
      "16-bit LoRA adapters: 640.00 MB\n",
      "Total QLoRA model: 4468.51 MB\n",
      "Equivalent fp16 model: 7476.51 MB\n",
      "Memory savings: 40.2%\n"
     ]
    }
   ],
   "source": [
    "# --- Memory Usage Comparison ---\n",
    "print(\"MEMORY USAGE COMPARISON\")\n",
    "\n",
    "# Calculate LoRA adapter memory\n",
    "lora_memory = sum(tensor.numel() * tensor.element_size() for tensor in adapter_weights.values())\n",
    "\n",
    "# Estimate base model memory (4-bit)\n",
    "base_memory = 0\n",
    "for name, param in base_model.named_parameters():\n",
    "    if 'weight' in name and not any(lora_name in name for lora_name in ['lora_A', 'lora_B']):\n",
    "        base_memory += param.numel() * param.element_size()\n",
    "\n",
    "# Estimate what full model would be in fp16\n",
    "full_model_memory = sum(p.numel() for p in base_model.parameters()) * 2  # fp16 bytes\n",
    "\n",
    "print(f\"Memory Usage:\")\n",
    "print(f\"4-bit base model: {base_memory / 1024**2:.2f} MB\")\n",
    "print(f\"16-bit LoRA adapters: {lora_memory / 1024**2:.2f} MB\")\n",
    "print(f\"Total QLoRA model: {(base_memory + lora_memory) / 1024**2:.2f} MB\")\n",
    "print(f\"Equivalent fp16 model: {full_model_memory / 1024**2:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - (base_memory + lora_memory) / full_model_memory) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23cdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
